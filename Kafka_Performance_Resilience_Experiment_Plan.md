# Kafka Performance & Resilience Experiment Plan

This plan outlines a comprehensive set of experiments to evaluate Apache Kafka under various use-case scenarios. Each experiment specifies the scenario conditions, the configuration parameters to vary (producer, consumer, and broker settings), and the metrics to collect (categorized by producer, consumer, and broker behavior). The goal is to cover high-throughput, low-latency, and fault-tolerance scenarios relevant to a production Kafka deployment. By following these experiments, we can identify performance bottlenecks and optimal configurations for different workloads ￼ ￼.

Experiment Scenarios Overview

The table below summarizes the planned experiment scenarios, key configuration variations, test conditions, and metrics to be gathered for each scenario:

Scenario (Use Case)	Key Configurations Varied (Producer / Consumer / Broker)	Test Conditions & Faults (Workload & Topology)	Metrics Collected (Producer ↔ Consumer ↔ Broker)


1. Sustained High-Volume Ingestion (Continuous heavy load)	Producer: acks (e.g. 1 vs all), linger.ms (0 vs 50ms), batch.size (e.g. 16KB vs 1MB), compression.type (none vs gzip) ￼ ￼.  Consumer: enable.auto.commit (on vs off), max.poll.records (e.g. 100 vs 1000).  Broker: replication.factor (1 vs 3), num.io.threads (e.g. 8 vs 16).	- Steady high throughput workload (e.g. multiple producers sending small messages continuously at max rate).  - 3 Kafka brokers, 1 topic with many partitions (e.g. 50-100 partitions) ￼.  - No fault injection (baseline throughput test).	- Producer metrics: send throughput (messages/sec, MB/sec), average produce request latency ￼, error/retry count.  - Consumer metrics: consume throughput (messages/sec), end-to-end latency (produce-to-consume) ￼, consumer lag.  - Broker metrics: CPU utilization, disk I/O throughput, network I/O, GC pauses, broker request latency.


2. Burst and Intermittent Traffic (Periodic bursts & idle periods)	Producer: linger.ms (0 for low latency vs higher to buffer bursts), batch.size (tuned along with linger).  Consumer: max.poll.records (smaller vs larger) to adjust batch processing, fetch.min.bytes (0 vs larger) ￼.  Broker: default configs (focus on traffic pattern).	- Bursty workload: producers alternate between high-rate bursts and idle periods (to simulate intermittent load).  - E.g. 5 producers send 10k msg/s for 10 seconds, then pause, repeat.  - 3 brokers, 1-2 topics, moderate partitions (e.g. 10 partitions).  - No fault injection (focus on handling bursts).	- Producer metrics: peak throughput during bursts, produce latency during burst vs normal.  - Consumer metrics: consumption rate vs burst input, consumer lag buildup and catch-up time, max poll batch size used.  - Broker metrics: request queue time, CPU spikes, any backpressure signs (e.g. TCP backlog, memory usage) during bursts.

3. Low-Latency Pipeline (Minimize end-to-end latency)	Producer: acks (compare all vs 1 for durability vs latency), linger.ms (0 to minimize delay), small batch.size (e.g. 0–8KB).  Consumer: fetch.min.bytes (0 for immediate fetch), enable.auto.commit (on vs off to measure commit latency).  Broker: replication.factor (1 vs 3 to see impact on latency), increase num.network.threads if needed for concurrency.	- Low latency workload: small messages (e.g. 100B–1KB), continuous flow.  - Aim for real-time pipeline: measure end-to-end latency at a moderate message rate.  - 3 brokers, 1 topic with enough partitions for parallelism (e.g. 10 partitions).  - No faults, network is normal (focus on baseline latency).	- Producer metrics: produce acknowledgment latency (time to ack) for acks=1 vs all ￼, request rate.  - Consumer metrics: end-to-end latency (production timestamp to consumption time) ￼, 99th percentile latency, consumer processing time per message.  - Broker metrics: request handling latency (produce/fetch request times), p99 broker latency, network round-trip times.


4. High Concurrency (Many Clients) (Many producers & consumers)	Producer: connection count (e.g. 50+ producer clients), possibly vary compression.type to save bandwidth.  Consumer: multiple consumer groups vs one group (to test load), max.poll.records tuned per group.  Broker: num.io.threads and num.network.threads (scale with client count), socket.request.max.bytes if needed.	- Concurrent clients: e.g. 50 producers and 50 consumers simultaneously on 5 topics.  - Each producer moderate rate (e.g. 1000 msg/s each) to simulate high total throughput.  - 3 brokers cluster, total partitions e.g. 100 (spread across topics).  - No explicit fault, but high connection count tests broker threading.	- Producer metrics: aggregate throughput across all producers, any send failures under load.  - Consumer metrics: throughput per consumer group, consumer lag distribution across many consumers, any rebalances observed.  - Broker metrics: CPU and memory usage scaling with connections, network bytes in/out, context-switch or thread usage (to see if num.io.threads sufficient), request queue latency under load.


5. Large Message Payloads & Compression (Throughput for big messages)	Producer: compression.type (none vs gzip vs LZ4), batch.size (e.g. 1MB or larger to allow big batches), linger.ms (tuned to batch big messages).  Consumer: fetch.max.bytes (ensure it can fetch large batches), max.poll.records (may be lower if messages are large).  Broker: message.max.bytes (ensure broker allows large messages), consider compression.type effect on broker CPU.	- Large message workload: e.g. message size 500KB – 1MB, moderate rate (to focus on throughput in MB/s).  - Fewer producers (e.g. 2–3) due to large message size but still aiming to saturate bandwidth.  - 3 brokers, replication factor 3 to see effect of replicating large payloads.  - No fault injection.	- Producer metrics: throughput in MB/s (data volume) ￼, compression ratio achieved, producer CPU usage (for compression).  - Consumer metrics: consumption rate (MB/s), decompression time if applicable, consumer memory (to handle large batches).  - Broker metrics: network throughput (MB/s) per broker, disk write throughput, broker CPU (compression overhead), any network saturation or IO bottlenecks.


6. Numerous Topics/Partitions (Scalability with many partitions)	Producer: partitioning strategy (round-robin vs keyed) to distribute load, possibly test single topic vs multiple topics.  Consumer: multiple consumer groups spread across many partitions, fetch.min.bytes possibly higher to efficiently fetch from many partitions.  Broker: total partition count (e.g. 500+ partitions across topics), log.retention.ms (very low in one test to force log cleanup), num.replica.fetchers if replication heavy.	- Scaling test: e.g. 50 topics × 20 partitions each (1000 partitions total) spread over 3 brokers.  - Moderate message size (e.g. 1KB) and rate across all partitions (to test metadata overhead and load distribution).  - Possibly run with short retention (e.g. 1 minute) in one run to simulate active log cleanup.  - No fault injection, focus on scalability limits.	- Producer metrics: overall throughput across all topics, any increased latency sending to large number of partitions (e.g. due to metadata updates).  - Consumer metrics: consumption throughput across many partitions, consumer lag per partition (check if some partitions lag behind), rebalance time if a consumer joins/leaves a large group.  - Broker metrics: memory usage (for indexes of many partitions), controller metrics (e.g. partition leadership imbalance), CPU usage, any throttle in request handling due to many partitions.


7. Broker Failure and Recovery (Broker crash and failover)	Producer: acks=all with min.insync.replicas configured (to ensure durability during failure) ￼, possibly retries and delivery.timeout.ms (to handle transient errors during failover).  Consumer: auto.offset.reset behavior (to verify if any offsets need reset after failure), enable.auto.commit (should commit periodically to avoid reprocessing).  Broker: replication.factor=3, one broker deliberately crashed; test with unclean.leader.election disabled (to avoid data loss).	- Failure test: 3-broker cluster, replication factor 3 for all topics ￼.  - Steady load (e.g. 1KB messages, moderate rate) on a topic.  - After ~5 minutes, kill 1 broker (simulate crash) for a period, then restore it.  - Possibly also test one broker soft failure (e.g. network isolate it) vs hard crash.	- Producer metrics: any increase in send errors or retries during failure, produce latency spike during leader re-election.  - Consumer metrics: consumption pause or lag spike when broker fails, time to catch up after new leader elected, any duplicate or missing messages observed.  - Broker/Cluster metrics: leader election time (how fast another broker takes over) ￼, in-sync replica count changes, data loss (should be zero with acks=all and ISR quorum), broker recovery time (time for the restarted broker to rejoin ISR).


8. Network Partition & Latency (Simulated network issues)	Producer: retries (high, to handle transient network errors), acks=all to ensure durability across partitions, request.timeout.ms (tuned if network is slow).  Consumer: fetch.max.wait.ms (may increase if simulating latency), session.timeout.ms (verify consumer group handling of delays).  Broker: no config change, but network impairment (latency, drop) introduced between brokers or between clients and cluster.	- Network fault test: introduce artificial network latency (e.g. add 100ms delay) or drop packets between one broker and others (partition the cluster or slow client-broker link).  - 3 brokers, replication factor 3. Continue running producers/consumers during the network issue.  - Possibly simulate a split-brain scenario (one broker isolated from cluster for some time) then heal network.	- Producer metrics: produce latency and error rate during high network latency periods, throughput drop due to retried requests.  - Consumer metrics: consumption throughput drop or stall during partition, consumer group rebalances if a broker becomes unresponsive, end-to-end latency increase.  - Broker metrics: inter-broker replication latency (lag between leader and follower replicas), request queue backlog on slow network links, controller events (if broker perceived as down).

(Table: Summary of experiment scenarios, configurations, conditions, and metrics.)

Each scenario above addresses a critical aspect of Kafka performance or reliability in production. The configuration dimensions (producer, consumer, broker settings) are varied to observe their impact. For example, in high-volume ingestion, we compare acks=1 vs acks=all (trading off durability for throughput) and test how enabling producer batching (linger.ms > 0, larger batch.size) can improve throughput ￼ ￼. In the low-latency scenario, we minimize batching and use acks=1 to favor latency at the cost of potential data loss, versus acks=all for full durability ￼. Consumer configurations like fetch.min.bytes and max.poll.records are tuned to improve fetch efficiency and reduce latency ￼. Broker-side changes such as increasing I/O threads or altering replication factor test how the cluster scales with load or handles failures.

Detailed Experiment Instructions per Scenario

Below are detailed instructions for each experiment scenario, including setup steps, configuration changes, and the specific metrics to record. All tests will be conducted on CloudLab (or a comparable testbed) with a configurable Kafka cluster, allowing controlled environment and fault injection.

1. Sustained High-Volume Ingestion (Continuous Load)

Purpose: Measure Kafka’s throughput limits under continuous heavy load, and observe how producer/broker configurations affect throughput and resource usage. This simulates a high-volume logging or metrics pipeline.

Setup & Workload:
	•	Cluster: 3 Kafka broker nodes (e.g., 3 m5.large or similar VMs) and ZooKeeper (if not using KRaft). Ensure high network and disk performance on brokers (to avoid external bottlenecks).
	•	Topic: 1 topic with a large number of partitions (e.g., 50 or 100 partitions) to maximize parallelism ￼. Replication factor can be 1 for initial tests (to measure raw throughput), then 3 for durability.
	•	Producers: Start multiple producer clients (e.g., 5–10 producers) sending messages as fast as possible. Use a fixed message size (e.g., 100 bytes or 1 KB) and no artificial delays between sends.
	•	Consumers: Start an equal number of consumer clients (5–10) in a consumer group to consume the topic. Ensure the consumer group can keep up (no backlog) to measure steady-state throughput (the highest rate where consumers don’t fall behind).

Configuration Variations: (test each combination methodically)
	•	Producer configs: Test with acks=1 vs acks=all ￼; linger.ms=0 (no batching) vs linger.ms=50ms (batch for 50ms); batch.size=16384 (16 KB, default) vs a larger e.g. 1048576 (1 MB) ￼; compression.type=none vs compression.type=gzip. These can be varied one at a time or in combination to see impacts. For example, start with baseline (acks=1, no linger, default batch, no compression), then add one change at a time to isolate effects.
	•	Consumer configs: Compare enable.auto.commit=true (with auto commit interval ~5s) vs false (manual commit after processing) to see if commit overhead affects throughput. Vary max.poll.records (e.g., 100 vs 1000) – a higher value lets consumer fetch larger batches of messages per poll which can improve throughput but might increase processing latency slightly ￼.
	•	Broker configs: If possible, run tests with different num.io.threads (e.g., 8 vs 16) or num.network.threads to see if broker thread pools affect throughput when many clients are active. Also test effect of replication factor: RF=1 vs RF=3 (with min.insync.replicas=2 for RF=3 to require at least 2 replicas ack for durability).

Metrics to Collect:
	•	Producer-side: Throughput in messages/sec and MB/sec (per producer and aggregated) ￼; average and 95th percentile produce latency (time for send() to be acknowledged) ￼; number of retries or errors (e.g., if any TimeoutException due to overload).
	•	Consumer-side: Throughput in messages/sec (per consumer and total); end-to-end latency (timestamp difference between produce and consume) for a sample of messages ￼; consumer lag (how far behind head of log each consumer is, ideally near zero in steady state).
	•	Broker-side: CPU usage (% per broker); network throughput (MB/s in, out per broker); disk I/O (write bytes/s, especially if not in-memory); any broker request queue latency or I/O thread idle time from Kafka JMX; Garbage collection pauses. Also monitor broker-level request latency (the time to serve produce/consume requests) as an indicator of stress ￼.

Record metrics for each configuration run. Each test run should last long enough (e.g., 5-10 minutes) to reach steady state throughput. We expect to identify which producer settings yield the highest throughput – e.g. enabling batching by setting linger.ms > 0 and increasing batch size can improve throughput by 10-20% by reducing requests ￼. Note any throughput drop when using acks=all (higher durability) compared to acks=1 ￼, and how much larger batches or compression might mitigate that. This scenario establishes a baseline throughput for comparison in later tests.

2. Burst and Intermittent Traffic (Bursty Load Patterns)

Purpose: Evaluate Kafka’s behavior under bursty traffic – periods of high volume followed by idle periods – which is common in workloads like periodic batch data uploads or sensor data spikes. We examine how well producers, consumers, and brokers handle sudden surges and whether any backpressure occurs.

Setup & Workload:
	•	Cluster: 3 brokers (similar setup as Scenario 1). Use a topic with a moderate partition count (e.g., 10 partitions, RF=3) – enough to distribute load but not as high as scenario 1.
	•	Producers: Use a smaller number of producers (e.g., 2–3) but program them to send bursts of messages. For example, each producer sends at a high rate (e.g., 10,000 messages/sec) for 10 seconds, then goes idle for 10 seconds, in a loop. This creates bursts separated by quiet periods.
	•	Consumers: Have consumers (2–3 in a group, same number as producers or more) continuously running. They will experience bursts of messages arriving, then none, etc. Ensure consumers do minimal processing so they can quickly catch up after bursts.

Configuration Variations:
	•	Producer configs: Test different batching strategies for bursts. E.g., linger.ms=0 (immediate send) vs linger.ms=100ms (which might batch messages within the burst period). A higher linger.ms could improve throughput during bursts but also introduce a slight delay – measure this trade-off. Keep acks=1 for this test for maximum throughput (since durability isn’t the focus here, but you can also note if acks=all changes burst handling).
	•	Consumer configs: Adjust max.poll.records. A higher max.poll (e.g., 500 or 1000) allows the consumer to grab a large batch of messages in one go after an idle period, which might help it catch up a burst faster. Conversely, a lower value (e.g., 100) means consumers will process in smaller chunks. Also test fetch.min.bytes=0 (immediate fetch) vs a larger value (e.g., 1 MB) which would make the broker wait to accumulate data – likely not ideal for burstiness, but worth observing effect ￼.
	•	Broker configs: Use default broker thread settings. Focus here is on network and queue behavior during bursts, not so much on thread tuning. However, ensure queued.max.requests (if using) is high enough so bursts don’t get dropped. You might also try enabling Kafka’s throttling (produce or consume quotas) in one run to see how it smooths bursts (optional).

Metrics to Collect:
	•	Producer-side: Throughput during burst periods (did all producers achieve ~10k msg/s as intended?); any send failures during peak (e.g., buffer full if brokers can’t keep up); produce latency distribution specifically during burst vs non-burst times.
	•	Consumer-side: How high does consumer lag spike during a burst (number of messages behind)? Measure the time it takes after a burst ends for the consumer lag to return to zero (catch-up time). Also track consumer throughput (messages/sec) – it will spike after each burst as consumers process backlog. Note if any consumers crash or rebalance (if they can’t keep up, they might fall out of the group).
	•	Broker-side: Monitor network I/O and I/O threads: during a burst, network bytes in/out should spike. Check broker CPU and disk usage during bursts vs idle (should also spike then drop). Critically, observe any broker request queue latency – e.g., if bursts cause queues to build up. Kafka’s JMX metrics like RequestHandlerAvgIdlePercent (which drops when busy) can indicate how saturated the broker thread pool is. If using quotas, check throttle metrics.

Procedure: Start the producers and consumers. Let the burst cycle run for a number of iterations (e.g., 5-10 minutes to capture multiple burst periods). Introduce the configuration changes one at a time or in combinations. For example, run with default linger.ms first, then with increased linger.ms, etc., while keeping other variables same. After each run, capture the metrics above.

Expectations: Kafka should handle bursts reasonably well if sized properly. We expect to see no data loss (all burst messages eventually delivered) but potentially increased latencies. Larger max.poll.records on consumers might reduce catch-up time since consumers can drain the burst backlog faster in one poll. Producer batching (linger.ms > 0) might slightly increase end-to-end latency for the burst (as it waits to batch) but could reduce the load on brokers by sending messages in fewer, larger requests. Watch for any broker-side warnings (e.g., if producers saturate network or if broker I/O threads become fully busy).

3. Low-Latency Pipeline (Real-Time Processing)

Purpose: Determine how to configure Kafka for minimal end-to-end latency. This scenario simulates a time-sensitive pipeline (e.g., financial transactions, tick data, or real-time analytics) where latency is more critical than absolute throughput.

Setup & Workload:
	•	Cluster: 3 brokers (or even 1 broker for a baseline, then 3 for replication impact). Use a topic with enough partitions (e.g., 10-20) to allow parallelism but not so many that it adds coordination overhead. Replication factor can be varied (RF=1 for lowest latency, RF=3 to test impact of replication).
	•	Producers: A few producers (1-3) sending messages at a moderate rate (for example, 1000 messages/sec each), with very small message sizes (100 bytes or 1KB). The idea is not to max out throughput, but to keep the pipeline busy enough to measure latency reliably.
	•	Consumers: Same number of consumers in a group (or even multiple consumer groups if you want to measure latency per group). Ensure consumers process messages immediately (maybe just count them, minimal logic) to focus on Kafka’s delivery latency, not processing time.

Configuration Variations:
	•	Producer configs: Minimize batching and waiting: linger.ms=0 (no delay) and use a small batch.size (maybe even the default 16KB or less) so that messages are sent virtually immediately. Use acks=1 for lowest latency (leader acknowledgment only). However, also test acks=all to see the latency penalty when waiting for replication ￼ (with a 3-broker RF=3 setup). If using acks=all, set min.insync.replicas=2 (so one follower must ack as well). Also consider enabling enable.idempotence=true (which is default in recent Kafka for acks=all) – it might add a tiny overhead but ensures no duplicates.
	•	Consumer configs: Use fetch.min.bytes=1 (immediate fetch) so consumers don’t wait to accumulate data. fetch.max.wait.ms can remain at default (~500ms) as it’s only relevant if min.bytes is high. max.poll.records – a lower value (like 100) might actually help keep latency low, as the consumer won’t try to batch too many messages at once (which could introduce queuing in the client). Also, if using manual commit, do it after each message or small batch to avoid long processing before commit (though committing each message might add overhead).
	•	Broker configs: For RF=3 test, ensure brokers are in the same region (no cross-datacenter latency). If aiming for absolute minimal latency, you might even test with unclean.leader.election.enable=false and focus on normal operation (leader not changing). Broker tuning: not much change needed, but you could ensure socket.buffer.size and socket.request.max.bytes are appropriately sized (though these mostly affect throughput). One important factor: controller failover or GC pauses could affect latency outliers, but those are complex to simulate; we’ll mainly measure steady-state latency.

Metrics to Collect:
	•	Producer-side: Produce latency – measure the time from calling send() to receiving acknowledgment. This is crucial. Kafka’s producer client metrics (if enabled) have request-latency-avg and request-latency-max. We want the distribution (avg, 95th, 99th percentile). Compare these between acks=1 vs acks=all. Also note throughput, but it will be moderate by design here.
	•	Consumer-side: End-to-end latency – we can have producers include a timestamp in each message, and measure at consumer the difference between consume time and produce time ￼. Gather average and p99 end-to-end latencies. Also ensure to measure if any consumer lag (should be near zero mostly). If any consumer rebalances occur (shouldn’t frequently here), note the pause it causes.
	•	Broker-side: Check p99 and p99.9 latency metrics on the broker (Kafka has metrics for local flush or request queue delay). Broker JMX has e.g. RequestLatencyMs for produce and fetch requests – collect those. Also see if CPU usage is low (it should be, since not saturating; if CPU is high, that could add latency). If RF=3, measure replication lag (follower fetch latency); ideally followers should be almost real-time. Network latency between brokers (if any monitoring) would be relevant if cluster spread out.

Execution: Run the test for a sufficient time (e.g., 5 minutes) to gather stable latency measurements. First, run with replication factor 1 (single broker, acks=1 basically) to measure the minimum achievable latency (no replication overhead). Then run with replication factor 3 and acks=all to measure the difference. Also try a run with acks=1 but RF=3 (so producers only wait for leader, not followers, but cost of replication is still in background) – this scenario might show how much just the act of replication in background affects consumer latency (though in theory, if followers can keep up, consumer reading from leader shouldn’t be affected except if the leader slows down waiting for disk flush or so).

We expect latencies on the order of a few milliseconds in the best case (likely single-digit ms average, tens of ms p99 if everything is local). With acks=all and RF=3, latencies will rise (the Red Hat experiment showed throughput drop with acks=all ￼, which correlates with higher latency per message). Document the latency impact of each config change.

4. High Concurrency (Many Producers and Consumers)

Purpose: Test Kafka’s ability to handle a large number of client connections and concurrent I/O. This simulates scenarios where many microservices or devices produce/consume simultaneously. We will examine broker resource use (threads, memory) and any contention as client count grows.

Setup & Workload:
	•	Cluster: 3 brokers (or possibly 5 brokers if available to distribute load, though 3 is fine to strain each). Use multiple topics to better distribute load among brokers (since each topic’s partitions will have leaders on different brokers). For example, 5 topics, each with 20 partitions, RF=3 – total 100 partitions across cluster.
	•	Producers: A high number of producer instances, e.g., 50 producers (if resources allow) or more. They can be on one machine or distributed across several (CloudLab might let you use multiple client nodes). Each producer sends at a moderate rate (e.g., 500 msg/s) so that aggregate incoming rate is high (50*500 = 25k msg/s). Use a mid-size message (e.g., 1 KB) to generate some network load.
	•	Consumers: Similarly, 50 consumer instances. These could be grouped differently: perhaps 5 consumer groups of 10 consumers each subscribing to the topics. That way, each group gets the full data (if topics are distinct per group) or you can have them all in one group for one topic to truly split partitions among many consumers. Define clearly: for example, you could have 5 topics, each topic is consumed by a separate group of 10 consumers (so 50 consumers total). This ensures all 100 partitions have a consumer, and 10 consumers per topic means some consumers share the load on that topic’s 20 partitions.

Configuration Variations:
	•	Producer configs: Mostly standard here, but you can introduce compression (since many producers might put pressure on network). Test with compression.type=none vs lz4 for instance, to see if compressing at producers reduces network load enough to improve throughput or CPU becomes a bottleneck. Keep acks=1 to avoid slowing down all producers with replication ACK (since focus is on connection scaling, not durability).
	•	Consumer configs: With many consumers, the main thing is how they partition the load. max.poll.records could potentially be high (like 500) since each consumer might be handling multiple partitions. Also ensure session.timeout.ms and heartbeat.interval.ms are at defaults – we might see if any heartbeats are missed when consumers are busy. If some consumers are slow, it could cause group rebalances; we might tweak these to see effect (but maybe leave this for observation rather than variation).
	•	Broker configs: This is key for concurrency. Increase num.network.threads and num.io.threads if needed to handle many connections – e.g., Kafka default might be around 3 network threads; consider upping to, say, 5 or 8. num.io.threads default ~8; consider 16. Also monitor the socket.receive.buffer.bytes and socket.send.buffer.bytes (OS socket buffers) – defaults might suffice, but keep an eye if a lot of context switching occurs. If we want to push, try runs with default threads vs increased threads to see improvement. Possibly also test effect of partition count: one run with fewer partitions (like 30 partitions total for all topics) vs 100 partitions, to see if many partitions (with many consumers) adds overhead in coordination.

Metrics to Collect:
	•	Producer-side: Confirm the total incoming message rate (sum of all 50 producers) achieved. Monitor if any producers report send failures (e.g., if broker side is overwhelmed, some sends might time out). Also track producer-side CPU on client machines if many producers per host (to ensure client isn’t bottlenecking).
	•	Consumer-side: Total consumption rate (should equal production rate if no backlog). Check if any consumer lag is growing – ideally, even with many consumers, each should keep up with its partitions. If some consumers are slower, you might see slight lag on those partitions. Also monitor if any consumer group rebalance events happen mid-test (that would indicate some consumer couldn’t keep up or missed heartbeats). You can detect rebalances by consumer logs or JMX (or by monitoring group coordinator logs on broker).
	•	Broker-side: This scenario will really test broker resource usage:
	•	CPU usage on brokers likely high due to many concurrent requests.
	•	Memory usage – particularly, the overhead of having 100+ active client connections (each socket and buffers). Kafka heap usage may rise; monitor if any garbage collection spikes.
	•	Network usage – aggregate MB/s in/out. With compression off vs on, network usage should differ (less with compression, but more CPU).
	•	Threading metrics: e.g., how busy are the network threads (Kafka may have JMX for idle percentage of network and I/O threads). If network threads are pegged, increasing them might help.
	•	Request handling time – check the produce and fetch request latency on brokers. Under heavy concurrent load, these might increase or become more variable.
	•	Possibly monitor the Linux system metrics too: context switches, file descriptors (to ensure we haven’t hit any FD limits with 100 connections).

Run this scenario for a decent duration (e.g., 10 minutes) to ensure steady state. First, run with baseline broker thread settings. Then, if results show high broker CPU or saturated threads, increase num.io.threads/network.threads and run again to compare metrics. The expectation is Kafka can handle dozens of clients easily, but we want to identify at what point adding more producers/consumers yields diminishing returns or instability. Document how throughput scales with client count and note any warning signs (like long GC pauses, frequent rebalances, etc.).

5. Large Message Payloads & Compression

Purpose: Assess performance when handling large messages, and the impact of compression on throughput and resource usage. In production, some use-cases involve big messages (e.g., images, JSON blobs). We need to understand Kafka’s throughput in MB/s and any strain on brokers with large payloads.

Setup & Workload:
	•	Cluster: 3 brokers (RF=3 to also gauge replication cost on large messages). If possible, use brokers with adequate disk and network capacity (large messages can quickly saturate network or disk). Ensure message.max.bytes broker config is set higher than the largest message size we will send (default is 1 MB; if testing up to 1MB, it’s fine, but if bigger, raise it).
	•	Topic: 1 topic with a moderate partition count (e.g., 6 partitions) – large messages themselves will use bandwidth; too many partitions might not increase throughput if network is the bottleneck. With 6 partitions and 3 brokers, each broker could lead 2 partitions, balancing load.
	•	Producers: Fewer producers (e.g., 2 or 3) since each is sending large data. We can have each producer target different partitions or even different topics to spread out. The send rate will be lower (maybe 100 messages/sec per producer) because each message is huge, but that might already be a lot of MB/s.
	•	Consumers: Matching number of consumers (2–3), each likely assigned one partition (since 6 partitions, and 2-3 consumers in group means some consume multiple partitions). They need to be able to process large messages (in memory) – ensure consumer JVM heap is sized accordingly. They will mostly just receive and discard the message (to simulate processing).

Configuration Variations:
	•	Producer configs: Compression: Test runs with compression.type=none, gzip, lz4 (and even zstd if available, though it’s slower). We expect compression to reduce network usage at cost of CPU. Use a relatively high batch.size (e.g., 1 MB or more) because we want the producer to potentially batch multiple large messages if possible (though if messages are themselves 1MB, batch might hold one at a time). linger.ms can be moderate (e.g., 10-50ms) to allow batching if messages are coming slightly slower. Keep acks=all in these tests to ensure all replicas get the data (since with big data, risk of loss is big; also it tests broker replication throughput).
	•	Consumer configs: Increase fetch.max.bytes (consumer max fetch size) to a value above the largest compressed message size (if message is 1MB and compressed, it could be smaller on the wire, but fetch.max.bytes should handle the full size when uncompressed). For example, set fetch.max.bytes=10MB to be safe. Also, max.partition.fetch.bytes on consumer should be large enough. max.poll.records can be low (like 1 or 5) so that a consumer doesn’t try to pull hundreds of large messages in one poll.
	•	Broker configs: The default replica.fetch.max.bytes and max.message.bytes should be reviewed. If messages are 1MB, default max.message.bytes=1MB might need bump to e.g. 2MB to be safe (since some overhead). Set replica.fetch.max.bytes similarly high so followers can replicate large messages. If testing >1MB messages, adjust accordingly. Also ensure broker JVM heap can handle buffering these messages (though Kafka mostly uses file-backed storage, large message might still use memory during transmission). Optionally, test with different compression.type at broker side (though broker just stores bytes; compression is client-side).

Metrics to Collect:
	•	Producer-side: Throughput measured in MB/sec is key here (as opposed to just msg/sec). See how compression affects this: e.g., with no compression, MB/sec to broker might be high; with compression, MB/sec on network is lower, but CPU usage on producer might rise. Collect producer CPU usage. Also measure produce request latency – large messages might have higher latency due to their size and compression time. Check if any RecordTooLargeException or similar errors occur.
	•	Consumer-side: Consumer throughput in MB/sec (should match producer if steady state). Measure consumer processing time per record (likely dominated by network I/O and maybe decompression). Check consumer memory/GC – large messages might cause spikes in memory usage when deserializing. Also note if any timeouts occur (e.g., a consumer might take time to process a batch of a few large messages, causing longer intervals between polls – ensure this doesn’t trigger a rebalance by exceeding max.poll.interval.ms).
	•	Broker-side: Network throughput – measure bytes in/out per second on each broker. With replication factor 3 and acks=all, each message is sent to 3 brokers (one as leader, then to 2 followers). So network in on leader and out to followers, etc. We should see roughly 3x the producer throughput in aggregate broker network (since data is replicated). Monitor disk throughput – writing large sequential append of log segments. It may not be the bottleneck if network is saturated first, but record it. CPU usage – brokers might have to compress/uncompress only if clients do (brokers actually don’t recompress; they store compressed batches as is, but might decompress for consumers if consumer lacks support or if doing message format conversion – assume modern Kafka and matching versions so this is minimal). Still, check CPU usage which might increase due to handling large byte arrays. If using gzip compression, expect higher CPU on producers and maybe slightly on consumers, whereas brokers mostly handle bytes.
	•	Also track memory (heap) on brokers – large messages could fill up network buffers or page cache. Kafka’s SocketServer might allocate buffers of size up to the message. Ensure no OOM occurs.

Procedure: Run with each compression setting for a fixed time (e.g., 5 minutes each) after things stabilize. Before switching compression, ensure to flush or use a new topic (to avoid mixing compressed/uncompressed data in one topic, which Kafka can handle, but measuring might be easier if separate runs). Possibly measure how enabling compression improves throughput in terms of MB/s delivered to brokers (it might actually reduce apparent throughput in MB/s at broker because data is smaller over network – but number of messages delivered could potentially increase if network was bottleneck). Also, note the trade-off: e.g., with no compression we might achieve X MB/s network usage and saturate it; with compression, network MB/s goes down (good) but CPU goes up and maybe overall messages/s can increase or not depending on CPU headroom.

We expect that large messages will often be network-bound. Kafka can handle them, but throughput (msgs/sec) will be low, though MB/sec could still be high. This test will clarify maximum payload handling. It’s also useful to see if there’s any instability (like if a broker falls behind in replication because of large messages).

6. Numerous Topics and Partitions (Scalability Test)

Purpose: Determine how Kafka performs as the number of topics and partitions increases. This scenario mimics a production environment with many distinct data streams. We are interested in metadata overhead, memory usage, and any impact on throughput/latency when scaling topics/partitions.

Setup & Workload:
	•	Cluster: 3 brokers. We will create a large number of topics/partitions such that total partitions are high (e.g., 500 or 1000 partitions cluster-wide). For example, 100 topics each with 10 partitions (total 1000 partitions), replication factor 3. Distribute these topics evenly across brokers (Kafka’s auto assignment should do this).
	•	Producers: We likely cannot produce high throughput to 1000 partitions simultaneously with limited client processes, but we can simulate load by sampling topics. For instance, have 10 producer threads, each responsible for 10 topics (100 topics covered) and send at some rate (e.g., 100 msg/s per topic). Alternatively, use one producer that round-robins among many topics (though a single producer serializing across 100 topics might not fully simulate concurrent access – better multiple producers).
	•	Consumers: Possibly set up consumer groups per topic (if treating each topic as separate workload). For test simplicity, maybe have 100 consumer threads, each consuming one topic (or fewer consumers if we assume not all topics active). This could be resource heavy; adjust depending on environment. Another approach: focus on broker-side metrics for this scenario (the clients just ensure each partition sees some activity, but not heavy).

Configuration Variations:
	•	Producer/Consumer configs: Not varying much here per se; use defaults or moderate settings because the focus is on quantity of partitions. Possibly test partitioning strategy: e.g., use round-robin partitioner vs key-based. But with so many partitions, round-robin is default. Ensure producers aren’t overwhelmed by metadata – you might increase metadata.max.age.ms (metadata refresh interval) to see if frequent metadata updates (due to many partitions) affect CPU.
	•	Broker configs: Broker memory (heap) is important; consider the effect of num.partitions and index cache. We could adjust log.retention.ms to a short period (like 1 minute or an hour) in one test run to force log segment deletions and see if that overhead affects performance while many partitions exist. Also, test with different controller configurations (e.g., if one broker is controller managing 1000 partitions, how’s its CPU?). If possible, vary the partition count: run one test with 100 partitions vs another with 1000, to compare metrics.

Metrics to Collect:
	•	Producer-side: If we have many producers, check if any are slowed by metadata handling. One sign could be high CPU in producer (since it must handle a large cluster metadata). Kafka client usually handles thousands of partitions fine, but it’s good to observe if any send latencies go up slightly for large cluster metadata. Also monitor throughput per topic if possible, to ensure all topics are receiving data.
	•	Consumer-side: If using one consumer per topic, measure their lag and throughput individually. Possibly more useful is to measure time to rebalance if a consumer group spanning many partitions changes. For example, take one consumer group that subscribes to, say, 50 topics (500 partitions) – measure how long a rebalance takes if one consumer joins or leaves. This could be significant, as more partitions = more coordination work. So, you might intentionally trigger a rebalance during the test (e.g., stop one consumer and start it again) and record the pause in consumption.
	•	Broker-side: Key metrics:
	•	Controller CPU usage: The controller broker manages metadata for all partitions. With 1000 partitions, see if CPU or Zookeeper load (if using ZK) is notable. If using KRaft (no ZK), check broker controller thread usage.
	•	Memory (heap) on brokers: each partition has an in-memory index for the active segment, etc. Many partitions = more memory. See if JVM old gen grows; check HeapUsed JMX.
	•	Partition spread: Ensure topics/partitions are evenly spread – check broker load (leaders per broker, etc.). Kafka JMX can show partition count per broker.
	•	Throughput/latency: With moderate per-partition traffic, see if any broker shows higher request latency handling many partitions. It might not be significantly different unless partitions idle vs active patterns cause any effect.
	•	Log cleanup overhead (if retention is low): If we set retention very low, logs will delete segments frequently. Monitor if that process (which runs per partition) causes any spikes in disk I/O or CPU. Possibly see LogCleaner metrics if log compaction is on (though we likely focus on delete policy).

Execution: Start with baseline where retention is default (like a day) just to measure overhead of many partitions without frequent deletions. Run producers/consumers for maybe 5 minutes stable. Then, if testing retention effect, shorten retention (e.g., 1 minute) and run long enough for some segments to get deleted (maybe run 10+ minutes to accumulate log). Alternatively, manually trigger log cleanup by reducing retention mid-test. Also, do the consumer rebalance test as mentioned: during the run, kill one consumer and restart it after a short pause, measuring the gap in processing on that consumer’s partitions.

This scenario likely won’t maximize throughput, but it will reveal scalability considerations (memory usage, controller performance, rebalance time). We expect Kafka to handle 1000 partitions but we might see increased memory and slightly longer operations like startup time or rebalance. Document any notable findings such as “Rebalance of 500 partitions took X seconds” or “Controller CPU went up to Y% managing 1000 partitions”.

7. Broker Failure and Recovery (Resilience Test)

Purpose: Validate Kafka’s fault tolerance by simulating broker failures, and measure the system’s ability to continue without data loss, as well as the performance impact during failover. This test ensures that with proper replication, Kafka meets high-availability needs ￼.

Setup & Workload:
	•	Cluster: 3 brokers (since replication factor 3 provides tolerance for one broker failure). All topics should be created with replication.factor=3 and min.insync.replicas=2 (meaning acks=all will require 2 brokers to ack, allowing one broker down and still operation).
	•	Producers: Could reuse a configuration from scenario 1 or 3 – e.g., a steady moderate throughput (say 1000 msg/s of 1KB messages) so that we have a continuous flow to observe during failure.
	•	Consumers: One consumer group consuming the topics continuously. If multiple topics, perhaps focus on one topic to inject failure for clarity, or just note that all topics behave similarly under failure.
	•	Failure event: Decide on failure type – a hard crash (kill the broker process or VM) is most straightforward. Schedule it at, say, 2 minutes into the test. The broker remains down for a couple of minutes, then is brought back up.

Steps:
	1.	Start the producers and consumers, ensure normal operation for a baseline period (a couple of minutes) with all brokers up.
	2.	Trigger the broker failure: stop one broker completely (simulate crash). Monitor how quickly Kafka detects this (should be within broker heartbeat timeout, usually default 10 seconds if using ZK, or similar in KRaft) and how leader election happens for that broker’s partitions.
	3.	Continue running with the broker down for a while (a few minutes) so the system is in a degraded state but functioning.
	4.	Restart the broker and allow it to rejoin the cluster. Kafka will begin replicating missing data to it (catch-up) and it will rejoin as follower for its partitions.

Configuration Variations:
	•	Producer configs: Use acks=all and retries high (e.g., retries=Integer.MAX_VALUE or a large number, and max.in.flight.requests.per.connection=1 to avoid reordering with retries if using idempotence). This ensures producers will wait out the failure and not drop messages. (Acks=1 could be tested to see data loss, but in production one would use acks=all for critical data; we can note difference: with acks=1, a broker crash causes those messages on that broker to be lost since they weren’t replicated).
	•	Consumer configs: auto.offset.reset should be set to “latest” ideally, so if a partition’s leader was lost and comes back, the consumer doesn’t rewind. Also consider enable.auto.commit – it’s fine to leave on (with interval 5s) to checkpoint progress. After failure, check if any rebalances occurred (they likely will when broker dies, because group sees partitions moved).
	•	Broker configs: Ensure unclean.leader.election.enable=false on brokers (this is default false in newer Kafka). This setting prevents choosing an out-of-sync replica as leader (which could cause data loss) – thus ensuring no data loss as long as one in-sync replica exists. If we wanted, we could test with it true to see what happens, but best practice is false ￼. Also, having proper min.insync.replicas=2 (with acks=all at producer) means even during the failure, the producer won’t get acknowledgment unless at least 2 brokers (the leader and one follower) wrote the data. This guarantees any committed message was on at least 2 brokers, so when one goes down, the data is safe on another.

Metrics/Observations:
	•	Producer-side: When the broker crashes, producers connected to that broker’s partitions will experience errors. With acks=all, they might see increased latency waiting for ack or even timeouts until the failover completes. Track the number of resend attempts or the time gap during failover (how long produce rate dips). Once failover done, producers should resume at normal throughput to the new leader. Record the duration of disruption (e.g., if throughput drops to zero for a few seconds).
	•	Consumer-side: Consumers will notice the broker failure via partition revocations/reassignments. Measure any gap in consumption: e.g., did consumption pause for a few seconds during rebalance? Consumer lag might spike for partitions that were on the down broker until they get a new leader. Check if any messages were duplicated or lost (with acks=all and proper config, there shouldn’t be lost messages; duplicates could occur if producers retried and the first attempt actually succeeded on leader just before crash – idempotent producer mitigates duplicates).
	•	Broker/Cluster: Key metric is failover time: how long from crash to new leader elected for each partition. This is usually on the order of a few seconds (depending on detection timeout). We can look at broker logs or JMX (Kafka has a metric for “UncleanLeaderElections” count – which should remain 0 if all goes well, and perhaps a metric for active controller events). Also monitor In-Sync Replicas (ISR) counts: right after the crash, ISR for those partitions will shrink (one broker out). When the broker comes back, those partitions’ ISR will grow as the broker catches up. The catch-up time can be observed: if the workload is heavy, how fast does the new broker replicate the backlog? Monitor replication metrics: e.g., bytes per second replicated, and if the broker’s network/disk is catching up.
	•	Additionally, measure throughput during failure: ideally, the overall consumer throughput might drop slightly (since one broker down = some partition leadership changes, maybe slight hiccup). But Kafka should maintain near normal throughput after failover. Compare the before, during, and after failure performance.
	•	If possible, also simulate a network partition of a broker instead of a crash (the network partition scenario #8 will handle that explicitly), but differences: a crash is a hard stop; a partition might not be detected as quickly if partial connectivity.

This scenario should demonstrate Kafka’s resilience: with proper config, no messages are lost, and downtime is minimal (the time for leader election). It will provide insight into how long recovery takes and if any performance degradation occurs during that window.

8. Network Partition & High Latency Simulation

Purpose: Assess Kafka’s behavior under network issues, such as increased latency or partial network failure. This test complements scenario 7 by focusing on network-induced faults (e.g., slow network, dropping packets) rather than a full broker crash. It helps ensure the system can tolerate network blips common in cloud environments.

Setup & Workload:
	•	Cluster: 3 brokers. Use a similar topic setup as scenario 7 (RF=3, so data is replicated).
	•	Producers/Consumers: A moderate continuous load (say 500 msg/s of 1KB, like earlier) so that timing effects can be observed. Even 1 producer and 1 consumer could suffice to illustrate increased latency, but using a few clients is better for averaging.
	•	Network impairment: Utilize Linux traffic control (tc) or CloudLab network shaping to introduce conditions:
	•	First test: add fixed latency on network links (e.g., +100ms one-way on the link to one broker or on all inter-broker communication).
	•	Second test: simulate a partition: e.g., drop packets between one broker and others (so that broker becomes isolated). Alternatively, simply shut off that broker’s network interface for a while which effectively partitions it (similar to crash from others’ perspective, except the broker itself might not know it’s isolated).

Configuration Considerations:
	•	Producer: High retries and delivery.timeout.ms to allow for delays. Possibly increase request.timeout.ms (the time a producer waits for ack) to not time out too quickly when latency is injected. For example, if default request.timeout is 30s, and we add 100ms latency, it’s fine; but if we added very high latency or simulate network hang, producers might hit timeouts. We can observe default first, then adjust if needed.
	•	Consumer: Might increase session.timeout.ms for consumer group if simulating a partition that causes delayed heartbeats, to avoid unnecessary rebalances. For example, if one broker is slow to respond, consumers connected to it might not get data quickly but still might heartbeat via another broker (heartbeats go to group coordinator, which might not be the partitioned broker). Anyway, keep defaults initially.
	•	Broker: No config changes, but note that if a broker is partially reachable, the controller might flag it as down if heartbeats lost. We could adjust broker.failure.detect.timeout (if using KRaft) or ZK session timeout for sensitivity, but better to use defaults to see how it naturally behaves.

Metrics to Collect:
	•	Producer-side: If adding latency, check produce latency – it will obviously rise roughly by the network delay added, plus any queuing. We measure how much (e.g., if 100ms added, does producer latency increase ~100ms?). Also see if throughput remains stable or dips (throughput might dip slightly if waiting longer for acks, but with pipeline and retries it might sustain). If a broker is partitioned, producers to that broker’s partitions will start timing out or blocking – measure how many messages got stuck/retried during that interval.
	•	Consumer-side: With uniform latency added, consumer end-to-end latency will increase similarly – measure that. If one broker isolated, any partitions it led will stop sending data to consumers until leader moves. Consumers might hang on those partitions then suddenly catch up after failover – similar to scenario 7, measure lag spike and recovery. Also check if consumer group had to rebalance due to broker partition (likely yes, if broker is deemed down).
	•	Broker-side: During high latency injection:
	•	Inter-broker replication: monitor if replication lag grows when latency is added between brokers. E.g., if one follower is 100ms away, its fetcher might fall a bit behind – check replica lag metrics.
	•	Network traffic: see if any sockets queue up (e.g., send/receive queue lengths in OS).
	•	If fully partitioned scenario: it’s effectively like broker crash from cluster’s viewpoint – measure detection time and failover like scenario 7.
	•	Possibly log/analyze any cluster metadata changes: e.g., controller might remove the partitioned broker from ISR after a timeout, then add back on heal.
	•	General: Monitor if any data loss occurred. With a network partition, if producers used acks=all and one broker was partitioned, those messages wouldn’t be acknowledged (since ISR shrinks), so producer should retry until failover. Ensure all messages eventually delivered.

Execution: For the latency injection test, apply the latency and run for say 5 minutes, then remove latency and see system returns to normal. For the partition test, isolate broker for maybe 1 minute then restore network, observing the effects.

Expected Behavior: Kafka should tolerate moderate latency without losing data, just higher end-to-end delays. For partition (which is like a temporary broker failure), the results should mirror scenario 7: the cluster reconfigures, then heals. We want to ensure that when network is restored, the broker catches up properly. Document any anomalies, e.g., if a partitioned broker, when coming back, maybe we see a burst of replication traffic.

Finally, after executing all these scenarios, we will have a comprehensive dataset of metrics. We will analyze throughput vs latency trade-offs, the impact of various configurations (as suggested by these experiments and external best practices ￼ ￼), and confirm Kafka’s reliability features under stress ￼. This thorough approach touches on all critical aspects: producer efficiency, consumer lag handling, broker resource usage, and fault tolerance, thereby giving a complete picture of Kafka performance in production-like conditions.